{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nfrom IPython.display import Audio\nimport random\nimport torch\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.get_device_name(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Dataset","metadata":{}},{"cell_type":"code","source":"# Paths for data.\nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nCrema = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n# IEMOCAP = \"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TESS","metadata":{}},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Joining them both\nTess_df = pd.concat([emotion_df, path_df], axis=1)\nTess_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=Tess_df, x='Emotions')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample Audio\npath = np.array(Tess_df.Path)[1]\ndata, sample_rate = librosa.load(path)\n\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=data, sr=sample_rate)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ravdess","metadata":{}},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\n\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Joining them both\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\n# changing integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=Ravdess_df, x='Emotions')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample Audio\npath = np.array(Ravdess_df.Path)[1]\ndata, sample_rate = librosa.load(path)\n\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=data, sr=sample_rate)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Crema-D","metadata":{}},{"cell_type":"code","source":"crema_directory_list = os.listdir(Crema)\n\nfile_emotion = []\nfile_path = []\n\nfor file in crema_directory_list:\n    # storing file paths\n    file_path.append(Crema + file)\n    # storing file emotions\n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_emotion.append('sad')\n    elif part[2] == 'ANG':\n        file_emotion.append('angry')\n    elif part[2] == 'DIS':\n        file_emotion.append('disgust')\n    elif part[2] == 'FEA':\n        file_emotion.append('fear')\n    elif part[2] == 'HAP':\n        file_emotion.append('happy')\n    elif part[2] == 'NEU':\n        file_emotion.append('neutral')\n    else:\n        file_emotion.append('Unknown')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Joining them both\nCrema_df = pd.concat([emotion_df, path_df], axis=1)\nCrema_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=Crema_df, x='Emotions')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample Audio\npath = np.array(Crema_df.Path)[1]\ndata, sample_rate = librosa.load(path)\n\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=data, sr=sample_rate)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Savee","metadata":{}},{"cell_type":"code","source":"savee_directory_list = os.listdir(Savee)\n\nfile_emotion = []\nfile_path = []\n\nfor file in savee_directory_list:\n    file_path.append(Savee + file)\n    part = file.split('_')[1]\n    ele = part[:-6]\n    if ele=='a':\n        file_emotion.append('angry')\n    elif ele=='d':\n        file_emotion.append('disgust')\n    elif ele=='f':\n        file_emotion.append('fear')\n    elif ele=='h':\n        file_emotion.append('happy')\n    elif ele=='n':\n        file_emotion.append('neutral')\n    elif ele=='sa':\n        file_emotion.append('sad')\n    else:\n        file_emotion.append('surprise')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Joining them both\nSavee_df = pd.concat([emotion_df, path_df], axis=1)\nSavee_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=Savee_df, x='Emotions')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample Audio\npath = np.array(Savee_df.Path)[1]\ndata, sample_rate = librosa.load(path)\n\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=data, sr=sample_rate)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combining All","metadata":{}},{"cell_type":"code","source":"# creating Dataframe using all the 4 dataframes we created so far.\ndata_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=data_path, x='Emotions')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path['Emotions'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"# Exploring labels","metadata":{}},{"cell_type":"code","source":"def waveplot(data, sr, emotion):\n    plt.figure(figsize=(10,4))\n    plt.title(emotion, size=20)\n    librosa.display.waveshow(data, sr=sr)\n    plt.show()\n    \ndef spectogram(data, sr, emotion):\n    x = librosa.stft(data)\n    xdb = librosa.amplitude_to_db(abs(x))\n    plt.figure(figsize=(11,4))\n    plt.title(emotion, size=20)\n    librosa.display.specshow(xdb, sr=sr, x_axis='time', y_axis='hz')\n    plt.colorbar()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fear","metadata":{}},{"cell_type":"code","source":"emotion = 'fear'\npath = np.array(data_path['Path'][data_path['Emotions']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Anger","metadata":{}},{"cell_type":"code","source":"emotion = 'angry'\npath = np.array(data_path['Path'][data_path['Emotions']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Disgust","metadata":{}},{"cell_type":"code","source":"emotion = 'disgust'\npath = np.array(data_path['Path'][data_path['Emotions']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Neutral","metadata":{}},{"cell_type":"code","source":"emotion = 'neutral'\npath = np.array(data_path['Path'][data_path['Emotions']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sad","metadata":{}},{"cell_type":"code","source":"emotion = 'sad'\npath = np.array(data_path['Path'][data_path['Emotions']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Happy","metadata":{}},{"cell_type":"code","source":"emotion = 'sad'\npath = np.array(data_path['Path'][data_path['Emotions']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing Error causing possibilities","metadata":{}},{"cell_type":"code","source":"data_path = data_path[data_path['Emotions'] != 'calm']\ndata_path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=data_path, x='Emotions')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Shuffling","metadata":{}},{"cell_type":"code","source":"data_path = data_path.sample(frac=1)\ndata_path.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path['Emotions'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Lets get even here... \n* We will use upsampling and downsampling both here\n* We will keep 1800 of other emotions of the dataset and increase the number of neutral emotions.\n* Let's remove the number of surprises we have for now...\n","metadata":{}},{"cell_type":"markdown","source":"## Deleting till 1800","metadata":{}},{"cell_type":"code","source":"# Emotions to reduce\nemotions_to_reduce = ['fear', 'angry', 'happy', 'disgust', 'sad']\n\n# Separate out the rows for emotions\nreduced_data = (\n    data_path[data_path['Emotions'].isin(emotions_to_reduce)]\n    .groupby('Emotions')\n    .head(1800)  # Keep only the first 1800 samples for these emotions\n)\n\n# Keep all samples for emotions not in emotions_to_reduce\nother_data = data_path[~data_path['Emotions'].isin(emotions_to_reduce)]\n\n# Combine the reduced emotions and the other emotions\ndata_path = pd.concat([reduced_data, other_data], ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path['Emotions'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path.Emotions.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing the Surprises","metadata":{}},{"cell_type":"code","source":"data_path = data_path[data_path['Emotions'] != 'surprises']\nsns.countplot(data=data_path, x='Emotions')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Duplicating Neutral Emotion Audio Samples\n* We can duplicate a few random audio samples of Neutral Emotion till the count reaches that of the others","metadata":{}},{"cell_type":"code","source":"neutral_data = data_path[data_path['Emotions'] == 'neutral']\nduplicates_needed = 1800 - len(neutral_data)\nneutral_augmented = neutral_data.sample(duplicates_needed, replace=True, random_state=42)\n\n# Combine the original and duplicated 'neutral' data\nbalanced_neutral_data = pd.concat([neutral_data, neutral_augmented])\n\n# Combine all the balanced data\ndata_path = pd.concat([reduced_data, balanced_neutral_data])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verifying Count","metadata":{}},{"cell_type":"code","source":"print(data_path['Emotions'].value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=data_path, x='Emotions')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"# Function for adding noise to audio\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\n# Function for stretching the audio\ndef stretch(data, rate):\n    return librosa.effects.time_stretch(data, rate=rate)\n\n# Function for shifting the audio\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\n# Function for adding pitch to audio\ndef pitch(data, sampling_rate, pitch_factor):\n    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor*12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GPU\n\n# def noise(data, noise_factor=0.005):\n#     noise = torch.randn_like(data) * noise_factor\n#     return data + noise.to(\"cuda\")\n\n# def stretch(data, rate):\n#     # Using torchaudio for time stretching\n#     return torchaudio.transforms.TimeStretch(rate)(data)\n\n# def shift(data, shift_max=1600):\n#     # Implement time shifting using torch.roll\n#     shift = torch.randint(-shift_max, shift_max, (1,))\n#     return torch.roll(data, int(shift), dims=0)\n\n# def pitch(data, sample_rate, pitch_factor=0):\n#     # Using torchaudio for pitch shifting\n#     return torchaudio.transforms.PitchShift(sample_rate, pitch_factor)(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking any example and checking for the techniques.\npath = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)\n# data and sample_rate = diff per path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Noise Injection","metadata":{}},{"cell_type":"code","source":"x = noise(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Stretching\n* Changing speed","metadata":{}},{"cell_type":"code","source":"x = stretch(data, 0.7)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Shifting","metadata":{}},{"cell_type":"code","source":"x = shift(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Pitch","metadata":{}},{"cell_type":"code","source":"x = pitch(data, sample_rate, pitch_factor=np.random.uniform(-5, 5))\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GPU Code\n\n## Noise\n# # Assuming data is a NumPy array\n# data_tensor = torch.from_numpy(data)\n\n# # If using GPU\n# data_tensor = data_tensor.to(\"cuda\")\n\n# x = noise(data_tensor)\n\n# # Convert back to NumPy array for plotting\n# x_numpy = x.cpu().numpy()\n\n# plt.figure(figsize=(14,4))\n# librosa.display.waveshow(y=x_numpy, sr=sample_rate)\n# Audio(x_numpy, rate=sample_rate)\n\n## Stretching: changing speed\n# data_tensor = torch.from_numpy(data).unsqueeze(0)\n\n# # If using GPU\n# data_tensor = data_tensor.to(\"cuda\")\n\n# x = stretch(data_tensor)\n\n# x_numpy = x.cpu().numpy()\n\n# plt.figure(figsize=(14,4))\n# librosa.display.waveshow(y=x_numpy, sr=sample_rate)\n# Audio(x_numpy, rate=sample_rate)\n\n## Shifting\n# # Assuming data is a NumPy array\n# data_tensor = torch.from_numpy(data).to(\"cuda\")\n\n# # Convert to NumPy (CPU) for `librosa` pitch shifting\n# data_numpy = data_tensor.cpu().numpy()\n# # Apply pitch shifting\n# shifted_data_numpy = librosa.effects.pitch_shift(data_numpy, sr=sample_rate, n_steps=2)\n\n# # Convert back to PyTorch tensor on GPU\n# shifted_data_tensor = torch.from_numpy(shifted_data_numpy).to(\"cuda\")\n\n# # Convert back to NumPy for visualization and playback\n# x_numpy = shifted_data_tensor.cpu().numpy()\n\n# # Plotting and playback\n# plt.figure(figsize=(14, 4))\n# librosa.display.waveshow(y=x_numpy, sr=sample_rate)\n# plt.title(\"Pitch Shifted Audio\")\n# plt.show()\n# Audio(x_numpy, rate=sample_rate)\n\n## Pitch\n# # Assuming data is a NumPy array\n# data_tensor = torch.from_numpy(data).to(\"cuda\")\n\n# # Define the shift amount (positive for right shift, negative for left shift)\n# shift_amount = int(0.1 * sample_rate)  # Shift by 0.1 seconds\n\n# # Apply time shifting on GPU\n# shifted_data_tensor = torch.roll(data_tensor, shifts=shift_amount)\n\n# # Convert back to NumPy for visualization and playback\n# x_numpy = shifted_data_tensor.cpu().numpy()\n\n# # Plotting and playback\n# plt.figure(figsize=(14, 4))\n# librosa.display.waveshow(y=x_numpy, sr=sample_rate)\n# plt.title(\"Time Shifted Audio\")\n# plt.show()\n# Audio(x_numpy, rate=sample_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"# Using GPU\ndef extract_features(data, sample_rate):\n    result = torch.tensor([]).to('cuda')  # Start with an empty tensor on GPU\n    \n    # ZCR\n    zcr = torch.tensor(librosa.feature.zero_crossing_rate(y=data).mean(axis=1)).to('cuda')\n    result = torch.cat((result, zcr), dim=0)  # Concatenate on GPU\n\n    # Chroma_stft\n    stft = torch.tensor(np.abs(librosa.stft(data))).to('cuda')\n    chroma_stft = torch.tensor(librosa.feature.chroma_stft(S=stft.cpu().numpy(), sr=sample_rate).mean(axis=1)).to('cuda')\n    result = torch.cat((result, chroma_stft), dim=0)\n\n    # MFCC\n    mfcc = torch.tensor(librosa.feature.mfcc(y=data, sr=sample_rate).mean(axis=1)).to('cuda')\n    result = torch.cat((result, mfcc), dim=0)\n\n    # Root Mean Square Value\n    rms = torch.tensor(librosa.feature.rms(y=data).mean(axis=1)).to('cuda')\n    result = torch.cat((result, rms), dim=0)\n\n    # MelSpectogram\n    mel = torch.tensor(librosa.feature.melspectrogram(y=data, sr=sample_rate).mean(axis=1)).to('cuda')\n    result = torch.cat((result, mel), dim=0)\n\n    return result.cpu().numpy()  # Convert back to NumPy array if needed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augment_and_extract_features(path, n_augment=100):\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # Extract features from the original audio\n    original_features = extract_features(data, sample_rate)\n\n    # Randomly choose an augmentation technique\n    augmentation_methods = [noise, stretch, shift, pitch]\n    method = random.choice(augmentation_methods)\n\n    # Apply the chosen augmentation\n    if method == pitch:\n        augmented_data = method(data, sample_rate, pitch_factor=np.random.uniform(-5, 5))\n    elif method == stretch:\n        augmented_data = method(data, rate=np.random.uniform(0.5, 1.5))\n    else:\n        augmented_data = method(data)\n\n    # Extract features from the augmented audio\n    augmented_features = extract_features(augmented_data, sample_rate)\n\n    # Stack original and augmented features vertically\n    combined_features = np.vstack((original_features, augmented_features))\n\n    return combined_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GPU\n\n# with tf.device('/device:GPU:0'):\n#     # Your model and data operations here\n#     # Lists to hold the features and corresponding labels\n#     X, Y = [], []\n\n#     # Loop through the dataset\n#     for path, emotion in zip(data_path.Path, data_path.Emotions):\n#         # Get features for the original and augmented audio\n#         features = augment_and_extract_features(path)\n\n#         # Append each set of features to X, and the corresponding emotion to Y\n#         for feature in features:\n#             X.append(feature)\n#             Y.append(emotion)\n\n#     # Convert to numpy arrays\n#     X = np.array(X)\n#     Y = np.array(Y)\n\n# # Shuffle the dataset\n# X, Y = shuffle(X, Y, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset processing\nX, Y = [], []\nfor path, emotion in zip(data_path.Path, data_path.Emotions):\n    features = augment_and_extract_features(path)\n    for feature in features:\n        X.append(feature)\n        Y.append(emotion)\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n# Shuffle the dataset\nX, Y = shuffle(X, Y, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)\nFeatures.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_path.shape)\ndata_path.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(Y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have applied data augmentation and extracted the features for each audio files and saved them.","metadata":{}},{"cell_type":"markdown","source":"As of now we have extracted the data, now we need to normalize and split our data for training and testing.","metadata":{}},{"cell_type":"code","source":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"splitting it in the ratio 3:1","metadata":{}},{"cell_type":"code","source":"# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making our data compatible to model.\nx_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## input split\nX = np.expand_dims(X, -1)\nX.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc = OneHotEncoder()\n# y = enc.fit_transform(data_path[['label']])\ny = enc.fit_transform(data_path[['Emotion']])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y.toarray()\ny.shape","metadata":{},"execution_count":null,"outputs":[]}]}